{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import click\n",
    "from functools import partial\n",
    "import logging as LOGGER\n",
    "from src.data.data import (\n",
    "    create_directory,\n",
    "    split_data,\n",
    "    preprocess_netflow_data,\n",
    "    preprocess_pcap_data,\n",
    "    prepare_netflow_sequantial_data,\n",
    "    prepare_pcap_sequantial_data,\n",
    ")\n",
    "LOGGER.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", level=LOGGER.INFO)\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Embedding, Dense, TimeDistributed, Dropout, Conv1D, Flatten, Convolution1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D, Lambda\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/aggregated_binetflows/ddos_1s.csv'\n",
    "packet_type = 'netflow'\n",
    "label_column='label'\n",
    "transition=0\n",
    "rnn_seq=10\n",
    "forward_predict=1\n",
    "standardize=True\n",
    "poly=False\n",
    "test_set_size=0.3\n",
    "random_seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-23 23:43:30,234 INFO Read 35590 records\n",
      "2019-10-23 23:43:30,235 INFO Preparing training and testing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3559.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/saed/env/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "if packet_type == \"netflow\":\n",
    "    data = preprocess_netflow_data(data, label_column, transition)\n",
    "else:\n",
    "    data = preprocess_pcap_data(data, label_column)\n",
    "\n",
    "data = data[sorted(data.columns, reverse=True)]\n",
    "\n",
    "LOGGER.info(f\"Read {len(data)} records\")\n",
    "LOGGER.info(f\"Preparing training and testing data ...\")\n",
    "\n",
    "if packet_type == \"netflow\":\n",
    "    x, y = prepare_netflow_sequantial_data(\n",
    "        data, rnn_seq, forward_predict, standardize, poly, transition )\n",
    "else:\n",
    "    x, y = prepare_pcap_sequantial_data(\n",
    "        data, rnn_seq, forward_predict, standardize, poly\n",
    "    )\n",
    "x_tr, x_te, y_tr, y_te = split_data(x, y, test_set_size, random_seed, stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sd_totbytes', 'sd_srcbytes', 'sd_packets', 'sd_duration', 's_totbytes',\n",
       "       's_state', 's_srcip', 's_srcbytes', 's_src_port>=1024',\n",
       "       's_src_port<1024', 's_src_ip_d', 's_src_ip_c', 's_src_ip_b',\n",
       "       's_src_ip_a', 's_packets', 's_duration', 's_dstip', 's_dst_port>=1024',\n",
       "       's_dst_port<1024', 's_dst_ip_d', 's_dst_ip_c', 's_dst_ip_b',\n",
       "       's_dst_ip_a', 'n_udp', 'n_tcp', 'n_src_port>=1024', 'n_src_port<1024',\n",
       "       'n_src_ip_na', 'n_src_ip_c', 'n_src_ip_b', 'n_src_ip_a',\n",
       "       'n_normal_rate', 'n_normal', 'n_icmp', 'n_dst_port>=1024',\n",
       "       'n_dst_port<1024', 'n_dst_ip_na', 'n_dst_ip_c', 'n_dst_ip_b',\n",
       "       'n_dst_ip_a', 'n_conn', 'n_background_rate', 'n_background',\n",
       "       'm_duration', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_': 1, 'sd_': 4, 'n_': 20, 's_': 19}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{l : len(data.columns[data.columns.str.contains(l)].values) for l in set([i[:i.find('_')+1]for i in data.columns[:-1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_': array(['m_duration'], dtype=object),\n",
       " 'sd_': array(['sd_totbytes', 'sd_srcbytes', 'sd_packets', 'sd_duration'],\n",
       "       dtype=object),\n",
       " 'n_': array(['n_udp', 'n_tcp', 'n_src_port>=1024', 'n_src_port<1024',\n",
       "        'n_src_ip_na', 'n_src_ip_c', 'n_src_ip_b', 'n_src_ip_a',\n",
       "        'n_normal_rate', 'n_normal', 'n_icmp', 'n_dst_port>=1024',\n",
       "        'n_dst_port<1024', 'n_dst_ip_na', 'n_dst_ip_c', 'n_dst_ip_b',\n",
       "        'n_dst_ip_a', 'n_conn', 'n_background_rate', 'n_background'],\n",
       "       dtype=object),\n",
       " 's_': array(['s_totbytes', 's_state', 's_srcip', 's_srcbytes',\n",
       "        's_src_port>=1024', 's_src_port<1024', 's_src_ip_d', 's_src_ip_c',\n",
       "        's_src_ip_b', 's_src_ip_a', 's_packets', 's_duration', 's_dstip',\n",
       "        's_dst_port>=1024', 's_dst_port<1024', 's_dst_ip_d', 's_dst_ip_c',\n",
       "        's_dst_ip_b', 's_dst_ip_a'], dtype=object)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{l : data.columns[data.columns.str.contains(l)].values for l in set([i[:i.find('_')+1]for i in data.columns[:-1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_in = Input(shape=(rnn_seq, 4), dtype=tf.float32)\n",
    "s_in = Input(shape=(rnn_seq, 19), dtype=tf.float32)\n",
    "n_in = Input(shape=(rnn_seq, 20), dtype=tf.float32)\n",
    "m_in = Input(shape=(rnn_seq, 1), dtype=tf.float32)\n",
    "sd_gru = Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True))(sd_in)\n",
    "n_gru = Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True))(n_in)\n",
    "s_gru = Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True))(s_in)\n",
    "m_gru = Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True))(m_in)\n",
    "all_gru = concatenate([sd_gru, n_gru, s_gru, m_gru])\n",
    "out = TimeDistributed(Dense(units=2 if transition == 0 else 4, activation='softmax', name='Output'))(all_gru)\n",
    "model = Model([sd_in, s_in, n_in, m_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "          loss=keras.losses.categorical_crossentropy,\n",
    "          metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2490 samples, validate on 1068 samples\n",
      "Epoch 1/10\n",
      "2490/2490 [==============================] - 7s 3ms/step - loss: 0.4746 - acc: 0.7702 - val_loss: 0.3852 - val_acc: 0.8452\n",
      "Epoch 2/10\n",
      "2490/2490 [==============================] - 1s 480us/step - loss: 0.3637 - acc: 0.8341 - val_loss: 0.3265 - val_acc: 0.8522\n",
      "Epoch 3/10\n",
      "2490/2490 [==============================] - 1s 483us/step - loss: 0.3156 - acc: 0.8631 - val_loss: 0.2906 - val_acc: 0.8638\n",
      "Epoch 4/10\n",
      "2490/2490 [==============================] - 1s 483us/step - loss: 0.2914 - acc: 0.8670 - val_loss: 0.2602 - val_acc: 0.8841\n",
      "Epoch 5/10\n",
      "2490/2490 [==============================] - 1s 483us/step - loss: 0.2740 - acc: 0.8749 - val_loss: 0.2566 - val_acc: 0.8865\n",
      "Epoch 6/10\n",
      "2490/2490 [==============================] - 1s 481us/step - loss: 0.2595 - acc: 0.8776 - val_loss: 0.2287 - val_acc: 0.8955\n",
      "Epoch 7/10\n",
      "2490/2490 [==============================] - 1s 482us/step - loss: 0.2401 - acc: 0.8924 - val_loss: 0.2160 - val_acc: 0.9002\n",
      "Epoch 8/10\n",
      "2490/2490 [==============================] - 1s 484us/step - loss: 0.2261 - acc: 0.8955 - val_loss: 0.1932 - val_acc: 0.9095\n",
      "Epoch 9/10\n",
      "2490/2490 [==============================] - 1s 485us/step - loss: 0.2057 - acc: 0.9070 - val_loss: 0.1722 - val_acc: 0.9243\n",
      "Epoch 10/10\n",
      "2490/2490 [==============================] - 1s 486us/step - loss: 0.2042 - acc: 0.9098 - val_loss: 0.1686 - val_acc: 0.9288\n"
     ]
    }
   ],
   "source": [
    "x_tr = [np.array(x_tr[:, :, :4]), np.array(x_tr[:, :, 4:4+19]), np.array(x_tr[:, :, 4+19:4+19+20]), np.array(x_tr[:, :, -1:])]\n",
    "x_te = [np.array(x_te[:, :, :4]), np.array(x_te[:, :, 4:4+19]), np.array(x_te[:, :, 4+19:4+19+20]), np.array(x_te[:, :, -1:])]\n",
    "res = model.fit(xxxx_tr, y_tr, epochs=10, batch_size=32, validation_data=(xxxx_te, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [Input(shape=(rnn_seq,)) for i in range(x_tr.shape[-1])]\n",
    "embs = [(Embedding(rnn_seq, 80))(inputs[i]) for i in range(x_tr.shape[-1])]\n",
    "grus = [Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True))(embs[i]) for i in range(x_tr.shape[-1])]\n",
    "all_gru = concatenate(grus)\n",
    "out = TimeDistributed(Dense(units=2 if transition == 0 else 4, activation='softmax', name='Output'))(all_gru)\n",
    "model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "          loss=keras.losses.categorical_crossentropy,\n",
    "          metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(x.shape[1:], dtype=tf.float32))\n",
    "model.add(Bidirectional(GRU(80, recurrent_dropout=0.25, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(units=2 if transition == 0 else 4, activation='softmax', name='Output')))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "          loss=keras.losses.categorical_crossentropy,\n",
    "          metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2490 samples, validate on 1068 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[5,0] = -1 is not in [0, 10)\n\t [[{{node embedding_130/embedding_lookup}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-c5514254c28a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/saed/env/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[5,0] = -1 is not in [0, 10)\n\t [[{{node embedding_130/embedding_lookup}}]]"
     ]
    }
   ],
   "source": [
    "res = model.fit([x_tr[:, :, i] for i in range(x_tr.shape[-1])], y_tr, epochs=10, batch_size=32, validation_data=([x_te[:, :, i] for i in range(x_te.shape[-1])], y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = model.predict(xxxx_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [Input(shape=(10, 1), dtype=tf.float32) for i in range(10)]\n",
    "rnn_units = [Bidirectional(SimpleRNN(10, recurrent_dropout=0.3, return_sequences=True))(inputs[i]) for i in range(10)]\n",
    "all_units = concatenate(rnn_units)\n",
    "out = TimeDistributed(Dense(units=2, activation='softmax', name='Output'))(all_units)\n",
    "model = Model(inputs, out)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "          loss=keras.losses.categorical_crossentropy,\n",
    "          metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (env @ saed)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
